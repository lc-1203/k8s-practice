# 数据库性能测试（mysql vs mariadb，云存储 vs 本地存储）

[toc]

## 数据库性能测试概述

### 测试目的

- 对比Mysql-xtrabackup主从集群与Mariadb-Galera集群两种方案的性能差异
- 对比本地存储与云存储的性能差异
- 对比文件存储与块存储的性能差异
- 对比阿里云块存储不同规格见的性能差异（高效云盘、SSD、ESSD）
- 对比Mariadb-Galera方案单点与集群的差异
- 对比Mariadb-Galera集群方案是否使用MaxScale读写分离的性能差异

### 压测环境

- 环境：
  - 服务器：阿里云计算网络增强型ecs.sn1ne.2xlarge，8 vCPU，16 GiB（生产环境同配置）
  - 存储：SSD 500G **16800** IOPS（生产环境同配置）
  - 集群：K8S 1.20
- 数据库性能：容器化构建，资源限制为4核8G
- 压测工具：Sysbench（容器化构建）
- 压测方案：
  - 数据准备：16线程，16表，每张表3000000数据级（总共约10G数据量，模拟生产环境）
  - 压测线程：16
  - 压测类型：综合读写，主键查询，索引更新
  - 压测时间：600秒

## K8S环境搭建

### Ansible一键部署k8s集群

- 安装Ansible

```shell
yum install ansible -y
```

- 配置免密

```
# 返回home
cd ~ 

# 生成密钥文件
ssh-keygen -t rsa
# 直接三次回车

#查看密钥文件
cd .ssh
ls -rtla

#传输到免密机器上
ssh-copy-id -i ~/.ssh/id_rsa.pub root@172.16.16.(200~203)
```

- 导入集群部署文件与安装包
- 部署

```shell
ansible-playbook -i hosts single-master-deploy.yml -uroot -k
```

![image-20211204134456020](https://lc-tc.oss-cn-shenzhen.aliyuncs.com/lc-images/20211204134456.png)

### k8s命令补全工具

```shell
yum install -y bash-completion
source /usr/share/bash-completion/bash_completion
source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ~/.bashrc
```

### Helm

- 下载及安装

```shell
# 下载（自行选择版本）
wget https://get.helm.sh/helm-v3.7.1-linux-amd64.tar.gz
# 解压
tar zxvf helm-v3.6.1-linux-amd64.tar.gz
# 安装
mv linux-amd64/helm /usr/local/bin/
# 验证
helm version
```

- 删除Helm使用时关于kubernetes文件的警告

```shell
chmod g-rw ~/.kube/config
chmod o-r ~/.kube/config
```

### 安装metrics-server

- 导入bitnami仓库

```shell
helm repo add bitnami https://charts.bitnami.com/bitnami
```

- 下载Chart包

```shell
# 查询chart
helm search repo bitnami

# 拉取chart包
helm pull bitnami/metrics-server
```

- 修改配置

```yaml
apiService:
  create: true    # 开启
--- 
extraArgs:        # 配置
  kubelet-insecure-tls: true
  kubelet-preferred-address-types: InternalIP
  metric-resolution: 15s
```

- 部署

```shell
cd ~/metrics-server/
helm install metrics-server -n kube-system .
```

- 验证

```shell
kubectl top pod -A
kubectl top nodes
```

### 部署Prometheus监控

- 导入Helm仓库（grafana、prometheus-community）

```shell
# 添加grafana和prometheus-community仓库（无响应时多尝试几次）
helm repo add grafana https://grafana.github.io/helm-charts
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
```

- 下载Chart包

```shell
helm pull grafana/grafana
helm pull prometheus-community/prometheus
helm pull prometheus-community/prometheus-mysql-exporter
```

- 修改配置

```shell
# 修改gcr镜像kube-state-metrics
vim ./charts/kube-state-metrics/values.yaml
registry.cn-zhangjiakou.aliyuncs.com/lc-sc/kube-state-metrics:v2.0.0

# 关闭持久化
vim value.yaml
# 搜索/persistent,关闭（false）
```

- 部署

```shell
cd ~/prometheus/prometheus
kubectl create ns prometheus
helm install prometheus -n prometheus .
```

### 安装Grafana

- 配置

```yaml
# vim values.yaml
---
# 设置密码
adminUser: admin
adminPassword: grafana
---
# 开启nodeport
service:
  enabled: true
  type: NodePort
  port: 80
  targetPort: 3000
  nodePort: 30130
---
# 关闭持久化（默认关闭）
```

- 部署

```shell
cd ~/prometheus/grafana/
helm install grafana -n prometheus .
```

- 阿里云安全组开启30130端口后，即可通过IP:30130访问grafana
- 配置Prometheus数据源，导入Node Exporter模版：8919

### 安装prometheus-mysql-exporter

- 配置数据库实例连接地址和帐号密码并安装
- Grafana导入MySQL Overview模版：7362

### 阿里云CSI存储组件

> https://github.com/kubernetes-sigs/alibaba-cloud-csi-driver

- 创建AK

```
kubectl -n kube-system create secret generic alibaba-addon-secret --from-literal='access-key-id=xxx' --from-literal='access-key-secret=xxx'
```

- 下载ACK动态供给及插件部署文件

```
wget https://raw.githubusercontent.com/kubernetes-sigs/alibaba-cloud-csi-driver/master/deploy/ack/csi-plugin.yaml
wget https://raw.githubusercontent.com/kubernetes-sigs/alibaba-cloud-csi-driver/master/deploy/ack/csi-provisioner.yaml
```

- 修改两个文件的配置

```
# 插入AK
---
            - name: MAX_VOLUMES_PERNODE
              value: "15"
            - name: SERVICE_TYPE       # 从这里之后插入AK字段
              value: "plugin"            
            - name: ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  key: access-key-id
                  name: alibaba-addon-secret
            - name: ACCESS_KEY_SECRET
              valueFrom:
                secretKeyRef:
                  key: access-key-secret
                  name: alibaba-addon-secret
# 更改为对应区域的内网镜像，将registry.cn-hangzhou替换为registry-vpc.cn-zhangjiakou
:%s/registry.cn-hangzhou/registry-vpc.cn-zhangjiakou/g
```

- 部署

```
kubectl apply -f csi-plugin.yaml
kubectl apply -f csi-provisioner.yaml
```

- 云盘挂载测试

```
# vim disk-nginx-sfs.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx-sfs-rwo
spec:
  selector:
    matchLabels:
      app: nginx
  serviceName: "nginx"
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:stable-alpine
        volumeMounts:
        - name: data
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "alicloud-disk-essd"
      resources:
        requests:
          storage: 20Gi
```

```
kubectl apply -f disk-nginx-sfs.yaml 
```

## 部署数据库

### 准备本地挂载目录

- ansible一键删除目录并重新创建目录

```
- hosts:
  - worknodes
  tasks:
  - name: Remove a directory if it exist
    file:
      path: /data/vol-mysql
      state: absent
  - name: Create a directory if it does not exist
    file:
      path: /data/vol-mysql
      state: directory
      mode: '0755'
```

```
ansible-playbook -i hosts recreate-vol-mysql-fold.yml
```

### 部署Mysql-xtrabackup主从集群（本地pv）

- sc

```
vim 1-storageclass.yaml
```

```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage-mysql
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Retain
```

- pv

```
vim 2-pv.yaml
```

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-vol-mysql-master1
  labels:
    pv: mysql
spec:
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage-mysql
  local:
    path: /data/vol-mysql
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - "master1"
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-vol-mysql-node1
  labels:
    pv: mysql
spec:
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage-mysql
  local:
    path: /data/vol-mysql
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - "node1"
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-vol-mysql-node2
  labels:
    pv: mysql
spec:
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage-mysql
  local:
    path: /data/vol-mysql
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - "node2"
```

- Mysql

```
vim 3-mysql.yaml
```

```
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: mysql
  name: mysql
  labels:
    app: mysql
data:
  master.cnf: |
    # Apply this config only on the master.
    [mysqld]
    log-bin
    max_connections = 10000
    character-set-server = utf8
    [client]
    default_character_set = utf8
  slave.cnf: |
    # Apply this config only on slaves.
    [mysqld]
    super-read-only
    max_connections = 10000
    character-set-server = utf8
    [client]
    default_character_set = utf8
---
apiVersion: v1
kind: Service
metadata:
  namespace: mysql
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  namespace: mysql
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Generate mysql server-id from pod ordinal index.
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          # Add an offset to avoid reserved server-id=0 value.
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          # Copy appropriate conf.d files from config-map to emptyDir.
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      - name: clone-mysql
        image: ist0ne/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Skip the clone if data already exists.
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          # Skip the clone on master (ordinal index 0).
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0
          # Clone data from previous peer.
          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          # Prepare the backup.
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 4
            memory: 8Gi
        livenessProbe:
          exec:
            command: ["mysqladmin", "ping"]
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            # Check we can execute queries over TCP (skip-networking is off).
            command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
      - name: xtrabackup
        image: ist0ne/xtrabackup:1.0
        ports:
        - name: xtrabackup
          containerPort: 3307
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql

          # Determine binlog position of cloned data, if any.
          if [[ -f xtrabackup_slave_info ]]; then
            # XtraBackup already generated a partial "CHANGE MASTER TO" query
            # because we're cloning from an existing slave.
            mv xtrabackup_slave_info change_master_to.sql.in
            # Ignore xtrabackup_binlog_info in this case (it's useless).
            rm -f xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            # We're cloning directly from master. Parse binlog position.
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm xtrabackup_binlog_info
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi

          # Check if we need to complete a clone by starting replication.
          if [[ -f change_master_to.sql.in ]]; then
            echo "Waiting for mysqld to be ready (accepting connections)"
            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done

            echo "Initializing replication from clone position"
            # In case of container restart, attempt this at-most-once.
            mv change_master_to.sql.in change_master_to.sql.orig
            mysql -h 127.0.0.1 <<EOF
          $(<change_master_to.sql.orig),
            MASTER_HOST='mysql-0.mysql',
            MASTER_USER='root',
            MASTER_PASSWORD='',
            MASTER_CONNECT_RETRY=10;
          START SLAVE;
          EOF
          fi

          # Start a server to send backups when requested by peers.
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 100Gi
      storageClassName: local-storage-mysql
      selector:
        matchLabels:
          pv: mysql
```

```
kubectl create ns mysql
kubectl apply -f 3-mysql.yaml 
```

- 创建用户及测试数据库

```
kubectl exec -it -n mysql mysql-0 -- bash
mysql -p

grant all ON *.* TO maxscale@'localhost' identified by 'maxscale@db' with grant option;
grant all ON *.* TO maxscale@'%' identified by 'maxscale@db' with grant option;
grant all ON *.* TO monitor@'localhost' identified by 'monitor@db' with grant option;
grant all ON *.* TO monitor@'%' identified by 'monitor@db' with grant option;
grant all ON *.* TO exporter@'localhost' identified by 'exporter@db' with grant option;
grant all ON *.* TO exporter@'%' identified by 'exporter@db' with grant option;
flush privileges;

create database test;
grant all ON *.* TO test@'localhost' identified by 'test' with grant option;
grant all ON *.* TO test@'%' identified by 'test' with grant option;
```

- maxscale

```shell
kubectl apply -f 4-maxscale.yaml 
```



### 安装Mariadb-Galera多主集群（本地pv）

- 创建sc

```
vim 1-storageclass.yaml
```

```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage-mariadb
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Retain
```

- pv

```
vim 2-pv.yaml
```

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-vol-mariadb-master1
  labels:
    pv: mariadb
spec:
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage-mariadb
  local:
    path: /data/vol-mariadb
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - "master1"
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-vol-mariadb-node1
  labels:
    pv: mariadb
spec:
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage-mariadb
  local:
    path: /data/vol-mariadb
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - "node1"
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-vol-mariadb-node2
  labels:
    pv: mariadb
spec:
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage-mariadb
  local:
    path: /data/vol-mariadb
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - "node2"
```

- 下载chart包

```
helm pull bitnami/mariadb-galera
```

- 修改配置

```
# root用户
# 备份用户
# 初始化脚本

# 持久化
# 同步模式由mariabackup改为rsync
# 资源限制
```

- maxscale

### 压测工具

- 部署

```
kubectl run sysbench-client --tty -i --restart='Always' --image  docker.io/severalnines/sysbench:latest
```

## 压测准备

### 重装数据库

- 重装mariadb

```shell
helm uninstall mariadb-galera -n mariadb
kubectl delete pvc -n mariadb data-mariadb-galera-0 data-mariadb-galera-1 data-mariadb-galera-2

# 本地存储需手动删除
kubectl delete pv local-vol-mariadb-master1 local-vol-mariadb-node1 local-vol-mariadb-node2 

cd ~/ansible-ops/ &&
ansible-playbook -i hosts recreate-vol-mariadb-fold.yml 

cd ~/mariadb/ &&
kubectl apply -f 2-pv.yaml 

cd ~/mariadb/mariadb-galera/ &&
helm install mariadb-galera -n mariadb .
```

- 重装mysql

```
cd ~/mysql-cluster/ &&
kubectl delete -f 3-mysql.yaml
kubectl delete pvc -n mysql data-mysql-0 data-mysql-1 data-mysql-2
kubectl delete pv local-vol-mysql-master1 local-vol-mysql-node1 local-vol-mysql-node2

cd ~/ansible-ops/ &&
ansible-playbook -i hosts recreate-vol-mysql-fold.yml 

    cd ~/mysql-cluster/ &&
    kubectl apply -f 2-pv.yaml &&
    kubectl apply -f 3-mysql.yaml

# 创建帐号
```

### 开启监控面板

#### mariadb

- maxctrl

```
while true ;do kubectl exec -it -n mariadb maxscale-0 -- maxctrl list servers; sleep 10;done;
```

- top

```
while true;do kubectl top pod -n mariadb && kubectl top pod |grep sysbench-client;sleep 10;done
```

- 存储空间

```
while true;do  ls -lh /data/vol-mariadb/data;sleep 10;done
while true;do  du -sh /data/vol-mariadb/data/*;sleep 10;done
```

#### mysql

- maxctrl

```shell
while true ;do kubectl exec -it -n mysql maxscale-0 -- maxctrl list servers; sleep 10;done;
```

- top

```shell
while true;do kubectl top pod -n mysql && kubectl top pod |grep sysbench-client;sleep 10;done
```

- 存储空间

```shell
while true;do du -sh /data/vol-mysql/mysql/*;sleep 10;done
```

### 准备测试模型

- 进入pod

```shell
kubectl exec -it sysbench-client -- bash
```

- 压测

```shell
# 准备数据
sysbench \
    --threads=16 \
    --mysql-host=maxscale.mariadb \
    --mysql-port=4006 \
    --db-driver=mysql \
    --time=600 \
    --report-interval=1 \
    --mysql-user=test \
    --mysql-password=test \
    --mysql-db=test \
    --rand-type=uniform \
    --tables=16  \
    --table_size=10000000 \
    oltp_read_write  prepare
    
# 执行：修改线程，将prepare改为run
sysbench \
    --threads=300 \
    --mysql-host=maxscale.mariadb \
    --mysql-port=4006 \
    --db-driver=mysql \
    --time=600 \
    --report-interval=1 \
    --mysql-user=test \
    --mysql-password=test \
    --mysql-db=test \
    --rand-type=uniform \
    --tables=16  \
    --table_size=10000000 \
    oltp_read_write  run

# 清理：将prepare改为clean

# 综合读写TPS：oltp_read_write
# 主键查询测试: oltp_point_select
# 更新索引字段：oltp_update_index
```

- mariadb多节点读写

```
sysbench \
    --threads=300 \
    --mysql-host=mariadb-galera.mariadb \
    --mysql-port=3306 \
    --db-driver=mysql \
    --time=600 \
    --report-interval=1 \
    --mysql-user=test \
    --mysql-password=test \
    --mysql-db=test \
    --rand-type=uniform \
    --tables=16  \
    --table_size=10000000 \
    oltp_read_write  run
```



- 进入mariadb控制台

```shell
kubectl exec -it -n mariadb mariadb-galera-0 -- bash
mysql -uroot -proot@db
use test;
analyze table sbtest1,sbtest2,sbtest3,sbtest4,sbtest5,sbtest6,sbtest7,sbtest8,sbtest9,sbtest10,sbtest11,sbtest12,sbtest13,sbtest14,sbtest15,sbtest16;
```

- 进入mysql控制台

```shell
kubectl exec -it -n mysql mysql-0 -- bash
mysql -p
use test;
analyze table sbtest1,sbtest2,sbtest3,sbtest4,sbtest5,sbtest6,sbtest7,sbtest8,sbtest9,sbtest10,sbtest11,sbtest12,sbtest13,sbtest14,sbtest15,sbtest16;
```



```shell
show global variables like 'max_prepared_stmt_count';
set global max_prepared_stmt_count=1048576;
```

## 执行测试

### 对比新旧maxscale对性能的影响（mariadb集群）

| 方法        | 综合读写 | 主键查询 | 索引更新 |
| ----------- | -------- | -------- | -------- |
| maxscale-旧 | 1260     | 25276    | 11744    |
| maxscale-新 | 1256     | 42622    | 11823    |
|             | -0.3%    | +68.6%   | +6.7%    |

结论：

- 两种方法**写入**性能无差异；
- maxscale-旧用于mariadb集群==读写分离未生效==

### 对比mysql集群与mariadb集群

| 方法        | 综合读写 | 主键查询 | 索引更新 |
| ----------- | -------- | -------- | -------- |
| mysql集群   | 938      | 44865    | 4625     |
| mariadb集群 | 1256     | 42622    | 11823    |
|             | +25.2%   | -5%      | +155.6%  |

结论

- 资源：
  - 内存：mariadb集群测试完毕均为4.8G左右；mysql测试结束后主节点接近==7.2G==，从节点5.4G
  - CPU：均可达到4G上限

- 性能，mariadb集群性能**提升明显**

### 对比云存储与本地存储（ESSD，200G）

> 注：云存储ESSD部署时需将用户/用户组设置为0

| 方法         | 综合读写 | 主键查询 | 索引更新 |
| ------------ | -------- | -------- | -------- |
| 本地存储ESSD | 1256     | 42622    | 11823    |
| 云存储ESSD   | 1266     | 40610    | 12710    |
|              | +0.8%    | -4.7%    | +7.5%    |

结论：

- 使用云存储总体**性能差异不大**

### 对比云存储：块存储ESSD和文件存储NAS

NAS：极速型NAS 高级型 600G

| 方法        | 综合读写 | 主键查询 | 索引更新 |
| ----------- | -------- | -------- | -------- |
| 块存储ESSD  | 1266     | 40610    | 12710    |
| 文件存储NAS | 1003     | 40845    | 9646     |
|             | -20.8%   | +0.6%    | -24.1%   |

- 结论
  - 文件存储初始==响应时间非常慢==，如数据准备时间需要730秒，而块存储仅需140秒
  - 文件存储测试==非常不稳定==，后续应**加大数据量级加长时间**测试，且间隔设置为1

## 测试方案2

测试方案：

- 16线程，16张表，每张表三百万行
- 4核
- 参数：db-ps-mode=disable，600秒，间隔1秒

- 类型：oltp_read_write，oltp_point_select，oltp_update_index

### 对比mysql集群与mariadb集群

| 方法        | 数据准备 | 综合读写 | 主键查询 | 索引更新 |
| ----------- | -------- | -------- | -------- | -------- |
| mysql集群   | 276      | 452      | 31721    | 1867     |
| mariadb集群 | 655      | 977      | 21617    | 9082     |
|             |          |          |          |          |

## 对比mysql主从集群与mariadb-galera集群（参考TiDB压测方案）

测试方案：

- 数据主备：16线程，16张表，每张表千万数据级
- 压测：300线程，600秒，间隔1秒
- 服务器性能: 计算网络增强型ecs.sn1ne.2xlarge，8核16G SSD500G（生产环境同配置）
- 数据库性能：限制在4核8G
- 类型：oltp_read_write，oltp_point_select，oltp_update_index

### 1.6亿数量级

#### mysql主从+读写分离 vs mariadb-galera+读写分离

| 方案                  | 综合读写     | 95%(ms) | 主键查询    | 95%(ms) | 索引更新     | 95%(ms) |
| :-------------------- | :----------- | :------ | :---------- | :------ | :----------- | ------- |
| mysql+读写分离【1】   | 123.90       | 4768.67 | 9051.00     | 86.00   | 1563.06      | 511.33  |
| mariadb+读写分离【2】 | 556.10       | 759.88  | 14691.52    | 59.99   | 5831.67      | 189.93  |
| **TPS 提升 (%)**      | **+348.93%** |         | **+62.32%** |         | **+273.05%** |         |



| 1.6亿数量级      | 综合读写     | 主键查询     | 索引更新     |
| :--------------- | :----------- | :----------- | :----------- |
| mysql主从        | 123.90       | 9051.00      | 1563.06      |
| mariadb多主      | 1225.33      | 31915.30     | 6242.27      |
| **TPS 提升 (%)** | **+888.97%** | **+252.62%** | **+299.36%** |



#### mariadb+读写分离 vs mariadb+多主同时读写

| 方案                  | 综合读写     | 95%(ms) | 主键查询     | 95%(ms) | 索引更新    | 95%(ms) |
| :-------------------- | :----------- | :------ | :----------- | :------ | :---------- | ------- |
| mariadb+读写分离【2】 | 556.10       | 759.88  | 14691.52     | 59.99   | 5831.67     | 189.93  |
| mariadb+同时读写【3】 | 1225.33      | 383.33  | 31915.30     | 19.65   | 6242.27     | 196.89  |
| **TPS 提升 (%)【2】** | **+120.34%** |         | **+117.23%** |         | **+7.04%**  |         |
| **TPS 提升 (%)【1】** | **+888.97%** |         | **+252.62%** |         | **+299.36** |         |

#### 与TiDB对比

| 方案                  | 综合读写     | 95%(ms) | 主键查询     | 95%(ms) | 索引更新    | 95%(ms) |
| :-------------------- | :----------- | :------ | :----------- | :------ | :---------- | ------- |
| mariadb+同时读写【3】 | 1225.33      | 383.33  | 31915.30     | 19.65   | 6242.27     | 196.89  |
| TiDB（16核）【4】     | 3848.63      | 106.75  | 267516.77    | 1.67    | 17821.1     | 25.74   |
| **TPS 提升 (%)**      | **+214.09%** |         | **+738.21%** |         | **+185.49** |         |

### 1.6千万数量级

#### mysql+读写分离 vs mariadb+读写分离

| 方案                  | 综合读写 | 95%(ms) | 主键查询 | 95%(ms) | 索引更新 | 95%(ms) |
| :-------------------- | :------- | :------ | :------- | :------ | :------- | ------- |
| mysql+读写分离【1】   | 198.83   | 3386.99 | 43068.59 | 28.16   | 830.96   | 1129.24 |
| mariadb+读写分离【2】 | 2138.56  | 325.98  | 33308.01 | 20.00   | 13339.05 | 87.56   |
| **TPS 提升 (%)**      | +975.57% |         | -29.30%  |         |          |         |

#### mariadb+读写分离 vs mariadb+同时读写

| 方案                  | 综合读写     | 95%(ms) | 主键查询    | 95%(ms) | 索引更新    | 95%(ms) |
| :-------------------- | :----------- | :------ | :---------- | :------ | :---------- | ------- |
| mariadb+读写分离【2】 | 821.76       | 559.50  | 20227.44    | 38.25   | 9905.31     | 108.68  |
| mariadb+同时读写【3】 | 2138.56      | 325.98  | 33308.01    | 20.00   | 13339.05    | 87.56   |
| **TPS 提升 (%)【2】** | **+160.24%** |         | **+64.67%** |         | **+34.67%** |         |
| **TPS 提升 (%)【1】** |              |         |             |         |             |         |

### 1.6百万数量级

#### mysql+读写分离 vs mariadb+读写分离

| 方案                  | 综合读写 | 95%(ms) | 主键查询 | 95%(ms) | 索引更新 | 95%(ms) |
| :-------------------- | :------- | :------ | :------- | :------ | :------- | ------- |
| mysql+读写分离【1】   | 350.51   | 1803.47 | 23692.82 | 61.08   | 1053.75  | 943.16  |
| mariadb+读写分离【2】 | 746.18   | 719.92  | 21112.25 | 37.56   | 11214.63 | 90.78   |
| **TPS 提升 (%)**      |          |         |          |         |          |         |

#### mariadb+读写分离 vs mariadb+同时读写

| 方案                  | 综合读写 | 95%(ms) | 主键查询 | 95%(ms) | 索引更新 | 95%(ms) |
| :-------------------- | :------- | :------ | :------- | :------ | :------- | ------- |
| mariadb+读写分离【2】 |          |         |          |         |          |         |
| mariadb+同时读写【3】 |          |         |          |         |          |         |
| **TPS 提升 (%)【2】** |          |         |          |         |          |         |
| **TPS 提升 (%)【1】** |          |         |          |         |          |         |
